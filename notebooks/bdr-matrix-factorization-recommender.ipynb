{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorzation models for recommendation\n",
    "\n",
    "The goal of this notebook is to get familiar with matrix factorization algorithms for recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import solutions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration and preparation\n",
    "Use case: we've taken the classic MovieLens data set, but we also gave it a twist to make it more applicable to the implicit rating use cases we encounter in the field. For instance, when we wish to recommend consumer products for a web store, we often do not have access to explicit user feedback (e.g. star ratings) and have to rely on historical purchase or click data only.\n",
    " \n",
    "Therefore, we've binairzed the orgininal data set: all movie ratings with three or more stars are converted to rating 1.0 and all other watched and non-watched to 0.0 or NaN (implicit rating scenario). In particular:\n",
    "* a 1.0 rating means that the user watched the trailer (impression) and subsequently also the whole movie (positive preference, implicit 1.0 rating)\n",
    "* a 0.0 rating (optionally available through `drop_negatives=False`) means the user watched the trailer (impression), but decided not to watch the full movie (negative preference, implicit 0.0 rating), \n",
    "* NaN means the user didn't watch the trailer (no impression, unknown preference, implicit NaN or 0.0 rating). The original star rating can be thought of as a confidence measure for each rating, with higher ratings implying higher confidence that the user-item preference is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset()\n",
    "df = data.get_ratings(unary=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code to get item titles in case you wish to view them\n",
    "item_desc = data.get_descriptions()\n",
    "item_desc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of ratings, unique users and items\n",
    "\n",
    "### IMPLEMENT ###\n",
    "n_ratings = NotImplemented\n",
    "n_users = NotImplemented\n",
    "n_items = NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:,} users, {:,} items, {:,} ratings\".format(n_users, n_items, n_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of users-per-item\n",
    "# plot distribution of items-per-user\n",
    "# assumption: data doesn't contain duplicate ratings\n",
    "# tip: use df.groupby()\n",
    "\n",
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "### IMPLEMENT ###\n",
    "NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of users per item constitutes a long-tailed distribution, resembling the exponential distribution. We have to take note of the fact that there are a few very popular items (head) and a lot of niche items (tail). If we want to use our recommender to attend users to these niche items, we might want to give these items more weight, or filter the head in a postprocessing step. For now, we will continue using all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sparsity of user-item rating matrix\n",
    "# ratio between number of rated user-item combinations and total user-item matrix size\n",
    "\n",
    "### IMPLEMENT ###\n",
    "sparsity = NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"user-item rating matrix sparsity: {:.2f} %\".format(sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# prepare data to a sparse matrix X\n",
    "train_sel = df['set'] == 'train'\n",
    "user_item = df.loc[train_sel, ['user','item']].values\n",
    "users, items = user_item[:,0], user_item[:,1]\n",
    "ratings = df.loc[train_sel, 'rating'].values\n",
    "\n",
    "X = csr_matrix((ratings, (users, items)), shape=(data.n_users, data.n_items))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "Now it's time to implement a _factorization_ algorithm, where we decompose the user-item rating matrix into latent factors of users and items. One particularly useful flavor is low-rank non-negative matrix factorization, where the solution is constrained to comprise non-negative numbers, as is the case with our rating prediction problem. Here, the user-item rating matrix $X$ is factorized as a product of two lower rank matrices $W$ and $H$:\n",
    " \n",
    "$$ X \\approx W H $$\n",
    "\n",
    "The low-rank property is very important, since it impplies that total information about the rating-property of the users is condensed in a much smaller volume of information. Thereby, the product of these lower-rank matrices will be less sparse that the original matrix, providing us with predictions for the unknown or zero entries. The rank is determined by the hyperparameter `n_components`.\n",
    "\n",
    "Let's fit a non-negative matrix factorization model using the `NMF` class from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "factorizer = NMF(n_components=20, random_state=42)\n",
    "W = factorizer.fit_transform(X)\n",
    "H = factorizer.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict top-3 item ratings for user 0\n",
    "# compute dot product of the specific user (W) and all item factors (H)\n",
    "\n",
    "### IMPLEMENT ###\n",
    "predicted_ratings = NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate top 3\n",
    "user0_top3 = np.argsort(predicted_ratings)[-3:][::-1]\n",
    "print(\"top-3 items for user 0: {}\".format([item_desc.loc[i] for i in user0_top3]))\n",
    "np.testing.assert_equal(solutions.user0_top3, user0_top3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frobenius norm of the difference between the reconstructed matrix and the original matrix $\\sqrt{\\sum\\limits_{i}\\sum\\limits_{j}(w_i h_j - x_{ij})^2}$ may be viewed as the matrix reconstruction error after compression in the lower-rank components. Let's implement it below. Note that, for the sake of similicity, we can implement an inefficient implementation that instantiates the full dense matrix $X$. In real-life situations, you may want to avoid this situation and write a more memory-efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(X, W, H):\n",
    "    \"\"\"frobenius norm of the matrix difference\"\"\"\n",
    "    ### IMPLEMENT ###\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = reconstruction_error(X.toarray(), W, H)\n",
    "print(\"reconstruction error: {:.2f}\".format(e))\n",
    "np.testing.assert_almost_equal(e, factorizer.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also plot the reconstruction error for different ranks of $W$ and $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_at_rank(n_components):\n",
    "    \"\"\"compute reconstruction error as a function of n_components\"\"\"\n",
    "    factorizer = NMF(n_components=n_components, random_state=42)\n",
    "    W = factorizer.fit_transform(X)\n",
    "    H = factorizer.components_\n",
    "    return factorizer.reconstruction_err_\n",
    "\n",
    "n_components = range(1, 42, 2)\n",
    "pd.Series([error_at_rank(n) for n in n_components], index=list(n_components)).plot()\n",
    "plt.xlabel('number of components (rank)')\n",
    "plt.ylabel('reconstruction error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the steepest decline in reconstruction error is due to the first 10 components. Let's use this number from now on. Of course we can tune this hyperparameter using cross-validation to get optimal results. Feel free to implement this step below, after we have introduced percentile ranks.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now we are going to do some predictions on the test set and evaluate recommender quality.\n",
    "\n",
    "An analysis of the distribution of the percentile ranks $rank_{ui}$ for all test items gives us an idea about the quality of recommendations. High percentile ranks (percentage of scores equal to or lower than $\\hat{x}$, so high scores get high percentile ranks) are considered as most desirable for the user, while the lowest possible percentile rank 0% is considered as least preferred.\n",
    "\n",
    "Since all items in the test set have been rated (purchased) by the user, our recommender is of good quality if most items in the test set have a percentile rank close to 100%. The average percentile rank of test items gives use a good indication of the overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorizer = NMF(n_components=10, random_state=42)\n",
    "W = factorizer.fit_transform(X)\n",
    "H = factorizer.components_\n",
    "\n",
    "def percentile_ranks(x):\n",
    "    \"\"\"convert array to descending percentile ranks\"\"\"\n",
    "    temp = x.argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(x))\n",
    "    ranks = ranks / (len(x) - 1) * 100\n",
    "    return ranks\n",
    "\n",
    "def item_percentile_rank(user_item, W, H):\n",
    "    \"\"\"for a single test item, compute the percentile rank using all item predictions\"\"\"\n",
    "    user, item = user_item[0], user_item[1]\n",
    "    x_pred = W[user,:].dot(H)\n",
    "    return percentile_ranks(x_pred)[item] \n",
    "\n",
    "user_item_test = df.loc[~train_sel, ['user','item']].values\n",
    "ratings_test = df.loc[~train_sel, 'rating'].values\n",
    "\n",
    "ranks = np.apply_along_axis(item_percentile_rank, axis=1, arr=user_item_test, W=W, H=H)\n",
    "print(\"average percentile rank: {:.2f}%\".format(ranks.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply signal detection theory to these percentile ranks, and obtain a receiver-operator-characteristic (ROC) curve for our recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.arange(0, 101, 1)\n",
    "def ranks_to_cdf(ranks):\n",
    "    \"\"\"convert percentile ranks to a cumulative distribution function\"\"\"\n",
    "    hist, _ = np.histogram(100 - ranks, normed=True, bins=100)\n",
    "    return np.insert(np.cumsum(hist), 0, 0)\n",
    "\n",
    "# results data frame\n",
    "results = pd.DataFrame(index=edges)\n",
    "\n",
    "# store results from a random recommender\n",
    "results['Random'] = ranks_to_cdf(np.random.rand(1000,) * 100)\n",
    "\n",
    "# store results of the NMF recommender\n",
    "results['NMF'] = ranks_to_cdf(ranks)\n",
    "\n",
    "def plot_cdf(df):\n",
    "    \"\"\"plot the columns of a data frame with percentiles as index\"\"\"\n",
    "    df.plot()\n",
    "    plt.title('Cumulative distribution function \\n of test item recommendation probability')\n",
    "    plt.xlabel('top % recommended')\n",
    "    plt.ylabel('probability (recall)')\n",
    "    plt.show()\n",
    "    \n",
    "plot_cdf(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "# compute the area-under-the-curve (AUC) scores for this model, to obtain a single metric that may be optimized\n",
    "# tip: use the results data frame from above\n",
    "# divide by 100 to map the percentages to proportions\n",
    "\n",
    "def area_under_the_curve(series):\n",
    "    \"\"\"area under the curve of cumulative distribution function of test item recommendation probability\"\"\"\n",
    "    ### IMPLEMENT ###\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = area_under_the_curve(results['NMF'])\n",
    "print(\"AUC score: {:.2f}\".format(auc_score))\n",
    "np.testing.assert_almost_equal(solutions.auc_score, auc_score, decimal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the most interesting part of this graph is located in the left lower corner. We usually don't have the oppertunity to recommend thousands of items to a user, but rather only a couple. Therefore, other metrics weight the items at the top of the list more than items at the bottom of the list. One of these metrics is the [Mean reciprocal rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank):\n",
    "$$\n",
    "MRR = \\frac{1}{|Q|}\\sum\\limits_{q=1}^{Q}{\\frac{1}{rank_q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean reciprocal rank of this model\n",
    "# note that we first have to convert back the descending percentile ranks to ascending absolute ranks\n",
    "\n",
    "def mrr(x):\n",
    "    \"\"\"mean reciprocal rank\"\"\"\n",
    "    x =  n_items - ((x / 100) * (n_items - 1))\n",
    "    \n",
    "    ### IMPLEMENT ###\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_score = mrr(ranks)\n",
    "print(\"mean reciprocal rank: {:.2f}\".format(mrr_score))\n",
    "np.testing.assert_almost_equal(solutions.mrr_score, mrr_score, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "First, we need a better baseline for our model. A very important baseline model is the popular item recommender. This  algorithm simply recommends the most popular items to all users. Usually in recommendation experiments, and in particular when dealing with implicit feedback data, this non-personalized model already has a fair amount of predictive power.\n",
    "\n",
    "Let's create some helper classes to wrap our factorizers and other recommender models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularItemsRecommender(Recommender):\n",
    "    \n",
    "    def fit(self, user_item, ratings):\n",
    "        users, items = user_item[:,0], user_item[:,1]\n",
    "        X = csr_matrix((ratings, (users, items)), shape=(self.n_users, self.n_items))\n",
    "        self.popularity = np.asarray(X.mean(axis=0)).flatten()\n",
    "        self.ranks = percentile_ranks(self.popularity)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, user_item):\n",
    "        items = user_item[:,1]\n",
    "        return np.array([self.popularity[i] for i in items])\n",
    "\n",
    "    def percentile_rank(self, user_item):\n",
    "        items = user_item[:,1]\n",
    "        return np.array([self.ranks[i] for i in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationRecommender(Recommender):\n",
    "    \n",
    "    def __init__(self, dims, factorizer):\n",
    "        super().__init__(dims)\n",
    "        self.factorizer = factorizer\n",
    "        \n",
    "    def fit(self, user_item, ratings, **kwargs):\n",
    "        users, items = user_item[:,0], user_item[:,1]\n",
    "        X = csr_matrix((ratings, (users, items)), shape=(self.n_users, self.n_items))\n",
    "        self.W = W = self.factorizer.fit_transform(X, **kwargs)\n",
    "        self.H = H = self.factorizer.components_\n",
    "        self.reconstruction_err_ = reconstruction_error(X.toarray(), W, H)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_item):\n",
    "        users, items = user_item[:,0], user_item[:,1]\n",
    "        ratings = np.zeros(len(users,))\n",
    "        for n, (user, item) in enumerate(zip(users, items)):\n",
    "            ratings[n] = self.W[user, :].dot(self.H[:, item])\n",
    "        return np.array(ratings)\n",
    "\n",
    "    def percentile_rank(self, user_item):\n",
    "        # note that this method uses the item_percentile_rank function defined in this notebook earlier\n",
    "        return np.apply_along_axis(item_percentile_rank, axis=1, arr=user_item, W=self.W, H=self.H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, results, name, **kwargs):\n",
    "    \"\"\"to fit a model and store the performance results\"\"\"\n",
    "    \n",
    "    print(\"\\n\", name)\n",
    "    \n",
    "    model.fit(user_item, ratings, **kwargs)\n",
    "    model.evaluate(user_item_test, ratings_test)\n",
    "    \n",
    "    ranks = model.percentile_rank(user_item_test)\n",
    "    results[name] = ranks_to_cdf(ranks)\n",
    "    \n",
    "    # optionally print additional metrics here\n",
    "    print(\"AUC score: {:.2f}\".format(area_under_the_curve(results[name])))\n",
    "    print(\"mean reciprocal rank: {:.2f}\".format(mrr(ranks)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "model = PopularItemsRecommender(dims=(data.n_users, data.n_items))\n",
    "results = fit_and_evaluate(model, results, name='PopularItems')\n",
    "\n",
    "factorizer = TruncatedSVD(n_components=30, random_state=42)\n",
    "model = FactorizationRecommender(dims=(data.n_users, data.n_items), factorizer=factorizer)\n",
    "results = fit_and_evaluate(model, results, name='TruncatedSVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with different factorizers and/or hyperparamter settings\n",
    "\n",
    "## IMPLEMENT ###\n",
    "# factorizer =\n",
    "# model = \n",
    "# results = fit_and_evaluate(model, results, name='MyModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FactorizationRecommender(dims=(data.n_users, data.n_items), factorizer=NMF(n_components=10, random_state=42))\n",
    "model.fit(user_item, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "### IMPLEMENT ###\n",
    "item_similarities = NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(item_similarities)\n",
    "plt.title('item-item similarity matrix')\n",
    "cb = plt.colorbar(fraction=0.03, pad=0.05)\n",
    "cb.ax.set_title('cosine \\n similarity \\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the clusters of similar movies, according to their latent factors. \n",
    "\n",
    "We can now inspect the top-n most similar items for a given item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "item = 10\n",
    "\n",
    "item_top = np.argsort(item_similarities[item,:])[-n:][::-1]\n",
    "print(\"top-{} most similar items for {}: \\n {}\".format(n, item_desc.loc[item], [item_desc.loc[i] for i in item_top]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a known issue that the latent factors are heavily biased by popularity. The quality of item-item similarities may be improved by removing the popularity effect from the user-item rating matrix, for instance using spectral clustering techniques. Google for more info and feel free to implement here to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization mechanics\n",
    "\n",
    "Now it's time to dive into the math of the matrix factorization algorithms.\n",
    "\n",
    "We find our decomposed matrix by minimizing the following cost function: \n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{2} \\times ||X - WH||_F^2+ \\frac{1}{2} \\times\\lambda \\times||W||_F^2+ \\frac{1}{2} \\times\\lambda \\times||H||_F^2$$\n",
    "\n",
    "The following computation yields the gradients:\n",
    "\n",
    "$$ \\nabla_{W}\\mathcal{L} = WHH^T-XH^T +  \\lambda  \\times W $$\n",
    "$$ \\nabla_{H}\\mathcal{L} = W^TWH - W^TX +  \\lambda  \\times H  $$\n",
    "\n",
    "We could of course implement gradient descent to solve (why not try later as exercise?), but we could also guess value of either W or H and then solve the quadratic problem for the other. So either we fix $H$ and solve $ \\nabla_{W}\\mathcal{L} = 0 $ or fix $W$ and solve $ \\nabla_{H}\\mathcal{L} = 0 $\n",
    "\n",
    "Both these equations are linear in the unknowns. At each step n, we apply the following transformations:\n",
    "\n",
    "$$ W_{(n+1)}=XH_{(n)}^T(H_{(n)}H^T_{(n)}+\\lambda \\mathbb{1})^{-1} $$\n",
    "$$ H_{(n+1)}=( W^T_{(n)}W_{(n)}+\\lambda \\mathbb{1})^{-1} W^T_{(n)}X $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class AlternatingLeastSquaresFactorizer(BaseEstimator, TransformerMixin):\n",
    "          \n",
    "    def __init__(self, n_components, n_iterations=10, l2=0.001):\n",
    "        \n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_components = n_components\n",
    "        self.l2 = l2\n",
    "          \n",
    "    def fit_transform(self, X, sample_weights=None):\n",
    "        X = X.toarray()\n",
    "        l2 = self.l2\n",
    "        n_components = self.n_components\n",
    "        n, m = X.shape\n",
    "        \n",
    "        # initate W and H as random matrices\n",
    "        W = np.random.rand(n, n_components)\n",
    "        H = np.random.rand(n_components, m)\n",
    "        \n",
    "        # store sample weights for later use\n",
    "        C = sample_weights\n",
    "        \n",
    "        # computation\n",
    "        for it in range(self.n_iterations):\n",
    "            \n",
    "            if sample_weights is None:\n",
    "                \n",
    "                W = np.dot(X, H.T).dot(np.linalg.pinv(np.dot(H, H.T) + l2 * np.eye(n_components)))\n",
    "                H = np.linalg.pinv(np.dot(W.T, W) + l2 * np.eye(n_components)).dot(np.dot(W.T, X))\n",
    "                \n",
    "            else:\n",
    "                ### IMPLEMENT ###\n",
    "                raise NotImplementedError('Weighted ALS not implemented yet.')\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print(\"iteration: {}\\t error: {:.4f}\".format(it, reconstruction_error(X, W, H)))\n",
    "        \n",
    "        self.components_ = H\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small test with random data\n",
    "model = AlternatingLeastSquaresFactorizer(n_iterations=201, n_components=20, l2=0.001)\n",
    "_ = model.fit_transform(csr_matrix(np.random.rand(200, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above custom `AlternatingLeastSquaresFactorizer` class works, but we've merely created something similar to (but more inefficient than) the scikit models. Let's customize our loss function to incoorpoate confidence information about the ratings (see [Hu et al. 2011](http://yifanhu.net/PUB/cf.pdf) for more information).\n",
    "\n",
    "Specifically, we can weigh the error with a confidence measure. A way to do this is by defining this confidence as a function of some user preference indicator, such as the number of purchases or views.  We can use the initial star ratings for this purpose. Let $c_{ij}$ be:\n",
    "\n",
    "$$c_{ij} = 1+ \\alpha r_{ij} $$\n",
    "\n",
    "Where $r_{ij}$ is the initial star rating of the user. The error is then weighted as follows:\n",
    "\n",
    "$$ ||X||_C = \\sum_{ij}C_{i,j} x_{i,j}^2 $$\n",
    "\n",
    "A quick calculation shows that the alternating least-square algorithm has the form,\n",
    "\n",
    "$$ W_{ij}^{(n+1)}=e_i^TXC^{(i)}H_{(n)}^T(H_{(n)}C^{(i)}H^T_{(n)}+\\lambda \\mathbb{1})^{-1}e_j $$\n",
    "$$ H_{ij}^{(n+1)}=e_i^T( W^T_{(n)}C^{(i)}W_{(n)}+\\lambda \\mathbb{1})^{-1} W^T_{(n)}C^{(i)}X e_j$$\n",
    "\n",
    "Try to implement the confidence weighted ALS algorithm in the `AlternatingLeastSquaresFactorizer` class above. We've already added a routine where the solver can be implemented. Good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare confidence measures to match ratings\n",
    "alpha = 40\n",
    "c = df.loc[train_sel, 'confidence'].values * alpha\n",
    "c = csr_matrix((c, (users, items)), shape=(data.n_users, data.n_items)).toarray()\n",
    "confidences = np.ones((data.n_users, data.n_items)) + c\n",
    "\n",
    "# define model\n",
    "params = dict(n_components=10, n_iterations=3, l2=0.001)\n",
    "model = FactorizationRecommender(dims=(data.n_users, data.n_items),\n",
    "                                 factorizer=AlternatingLeastSquaresFactorizer(**params))\n",
    "\n",
    "results = fit_and_evaluate(model, results, name='WeightedALS', sample_weights=confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the models, customize them, tune them, and try to beat the vanilla `NMF` with 10 components.\n",
    "\n",
    "## Big data libraries\n",
    "\n",
    "Obviously, these models thrive with more data, and we've only explored the smallest release of the MovieLens dataset. Do note that the computation of the matrix factorization models quickly increases with more data. Therefore, it is advised to implement these models using scalable machine learning frameworks such as Apache Spark or TensorFlow. Here are a couple of links to get you started with matrix factorization models using these frameworks.\n",
    "\n",
    "### Spark\n",
    "* https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html\n",
    "* http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html\n",
    "\n",
    "### TensorFlow\n",
    "* http://willwolf.io/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/  \n",
    "  Note that [Network architecture #1](http://willwolf.io/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/#Network-#1) shows our matrix factorization algorithm in neural network form.\n",
    "* http://katbailey.github.io/post/matrix-factorization-with-tensorflow/\n",
    "\n",
    "Feel free to try either one of these libraries and compare with this notebook.\n",
    "\n",
    "That's it.\n",
    "\n",
    "We hope you liked this notebook!\n",
    "\n",
    "Cheers,\n",
    "\n",
    "The hands-on Data Scientists from [BigData Republic](https://www.bigdatarepublic.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:recsys]",
   "language": "python",
   "name": "conda-env-recsys-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
